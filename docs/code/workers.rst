
Workers
^^^^^^^

TokenBucket
"""""""""""

We try to be nice citizens and crawl the website of the european parliament responsibly. The token-bucket-algorithm `<https://en.wikipedia.org/wiki/Token_bucket>`_ is used to generate crawling tokens at a steady pace which are then stored in a Python multiprocessing queue with a fixed size.

This queue is consumed by every crawler-instance individually. The crawler then blocks the execution until it was able to obtain one of these tokens.
It is therefore possible to rate limit all crawling functions by adjusting the token production rate.

.. autoclass:: europarl.workers.TokenBucketWorker
    :members:
    :undoc-members:


SessionPoller
"""""""""""""

This process determines all dates of parliamentary sessions by querying the parliaments website for a protocol. The resulting information is stored in the SessionDay table.

.. autoclass:: europarl.workers.SessionDayChecker
    :members:
    :undoc-members:

DateURLGenerator
""""""""""""""""

The URL generator supplies the crawlers with URLs to crawl via a queue. It gets these URLs either by querying the URLs table for entries which must be crawled and if there aren't any left, by generating them.

URLs are to be crawled if they are marked as not crawled yet or if they have to be recrawled and the recrawl_after-timestamp is in the past.
URLs are generated by applying a set of rules to the dates stored in the SessionDate table and stored as not crawled.

Every URL get's marked as enqueued when it gets added to the queue to prevent enqueueing it multiple times. The enqueued_at timestamp is used to determine which urls should be rescheduled because they are queued for a long time without succeeding. The created timestamp and enqueued_times allows to identify old and silently failing urls.

.. autoclass:: europarl.workers.DateUrlGenerator
    :members:
    :undoc-members:

Crawlers
""""""""

Crawler Instances are responsible for downloading documents from the provided URLs, store them on the filesystem, identify links to other documents (if the downloaded document is a .html-file) and store the success in the URLs table.
Found URLs are appended to the URLs table too.
