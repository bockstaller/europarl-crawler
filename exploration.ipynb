{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duplicate document comparison\n",
    "\n",
    "Each parlamentary session produces: \n",
    "- different types of documents (for example a session protocol and a word protocol) \n",
    "- in one (the word protocol is a singular mulitlingual document without translations) or more languages (the session protocol is translated into 24 languages) \n",
    "- in different file formats (.pdf, .html and .doc)\n",
    "\n",
    "This document compares the already scraped session and word protocols.\n",
    "\n",
    "Each were scraped as a .html- and .pdf-file in English and German.\n",
    "\n",
    "Unfortunately there is some data missing for the pdf-files, as the postprocessing process for them hasn't finished yet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from europarl.db import DBInterface\n",
    "import configparser\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.cbook as cbook\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "from matplotlib.pyplot import figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('settings.ini')\n",
    "dbi = DBInterface(config=config[\"General\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_length_per_session_date(rules):\n",
    "    results = {}\n",
    "    for rule in rules :\n",
    "        query = \"\"\" SELECT char_length(data ->> 'content') as fs, data ->> 'session_date' as date\n",
    "                    FROM documents\n",
    "                    WHERE data ->> 'rulename' =%s\n",
    "                    ORDER by date DESC\n",
    "                \"\"\"\n",
    "        with dbi.cursor() as db:\n",
    "            db.cur.execute(\n",
    "                query,\n",
    "                [rule,],\n",
    "            )\n",
    "            res = db.cur.fetchall()\n",
    "\n",
    "        res = list(zip(*res))\n",
    "\n",
    "        x = [dt.datetime.strptime(d,'%Y-%m-%d').date() for d in res[1]]\n",
    "        y = [int(value or 0) for value in res[0]]\n",
    "\n",
    "        results[rule]=(x,y)\n",
    "    return results\n",
    "        \n",
    "def plot_text_length_per_session_date(data, rules):\n",
    "\n",
    "    years = mdates.YearLocator(5)   # every year\n",
    "    year = mdates.MonthLocator(1)  # every month\n",
    "    years_fmt = mdates.DateFormatter('%Y')\n",
    "\n",
    "    fig, ax = plt.subplots(4)\n",
    "    fig.set_size_inches(18, 16)\n",
    "    \n",
    "    plt.subplots_adjust(hspace=0.4)\n",
    "\n",
    "    datemin, datemax = None, None\n",
    "\n",
    "    for index, rule in enumerate(rules):\n",
    "        x,y = data[rule]\n",
    "        ax[index].plot(x,y)\n",
    "\n",
    "        if datemin is None:\n",
    "            datemin = min(x)\n",
    "        elif min(x) < datemin:\n",
    "            datemin = min(x)\n",
    "\n",
    "        if datemax is None:\n",
    "            datemax = max(x)\n",
    "        elif max(x) > datemax:\n",
    "            datemax = max(x)\n",
    "\n",
    "        ax[index].set_title(rule)\n",
    "        ax[index].set(ylabel='characters per document', xlabel='date of the document')\n",
    "        ax[index].xaxis.set_major_locator(years)\n",
    "        ax[index].xaxis.set_major_formatter(years_fmt)\n",
    "        ax[index].xaxis.set_minor_locator(year)\n",
    "        ax[index].set_xlim(datemin, datemax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing protocol character counts\n",
    "\n",
    "\n",
    "Session protocols are available in .html starting in spring 2003 and as .pdf-files all the way back to winter 1994.\n",
    "\n",
    "Word procotcols are available in both formats back to winter 1994"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = ['protocol_en_html', 'protocol_de_html', 'protocol_en_pdf', 'protocol_de_pdf']\n",
    "\n",
    "data = get_text_length_per_session_date(rules)\n",
    "plot_text_length_per_session_date(data, rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = ['word_protocol_en_html', 'word_protocol_de_html', 'word_protocol_en_pdf', 'word_protocol_de_pdf']\n",
    "\n",
    "data = get_text_length_per_session_date(rules)\n",
    "plot_text_length_per_session_date(data, rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "def get_texts(rules):\n",
    "    query = \"\"\" SELECT res1.date, res1.text, res2.text FROM\n",
    "                    (\n",
    "                        SELECT data ->> 'content' as text, data ->> 'session_date' as date \n",
    "                        FROM documents \n",
    "                        WHERE data ->> 'rulename' = %s\n",
    "                    ) res1\n",
    "                JOIN (\n",
    "                        SELECT data ->> 'content' as text, data ->> 'session_date' as date \n",
    "                        FROM documents \n",
    "                        WHERE data ->> 'rulename' = %s\n",
    "                    ) res2\n",
    "                ON res1.date = res2.date\n",
    "                ORDER by res1.date DESC\"\"\"\n",
    "         \n",
    "    \n",
    "    with dbi.cursor(name='exploration', withhold=True) as db:\n",
    "        db.cur.execute(\n",
    "            query,\n",
    "            [rules[0],rules[1]],\n",
    "        )\n",
    "        yield from db.cur\n",
    "\n",
    "def compute_similiarities(rule_pairs):\n",
    "    results = {}\n",
    "    for pair in rule_pairs:\n",
    "        texts = get_texts(pair)\n",
    "        dates =[]\n",
    "        similiarity = []\n",
    "\n",
    "        name = pair[0] + \" \" + pair[1]\n",
    "\n",
    "        for i, res in enumerate(texts):\n",
    "                \n",
    "            dates.append(dt.datetime.strptime(res[0],'%Y-%m-%d').date())\n",
    "            similiarity.append(SequenceMatcher(None, (res[1] or \" \"), (res[2] or \" \")).real_quick_ratio())\n",
    "\n",
    "        results[name]={'dates': dates, 'similiarity': similiarity}\n",
    "        print(\"Computed {}\".format(name))\n",
    "\n",
    "    return results  \n",
    "\n",
    "def plot_text_similiarity(data):\n",
    "\n",
    "    years = mdates.YearLocator(5)   # every year\n",
    "    year = mdates.MonthLocator(1)  # every month\n",
    "    years_fmt = mdates.DateFormatter('%Y')\n",
    "\n",
    "    fig, ax = plt.subplots(4)\n",
    "    fig.set_size_inches(18, 16)\n",
    "    \n",
    "    plt.subplots_adjust(hspace=0.4)\n",
    "\n",
    "    datemin, datemax = None, None\n",
    "\n",
    "    for index, key in enumerate(data):\n",
    "        x = data[key]['dates']\n",
    "        y = data[key]['similiarity']\n",
    "        ax[index].scatter(x,y)\n",
    "\n",
    "        if datemin is None:\n",
    "            datemin = min(x)\n",
    "        elif min(x) < datemin:\n",
    "            datemin = min(x)\n",
    "\n",
    "        if datemax is None:\n",
    "            datemax = max(x)\n",
    "        elif max(x) > datemax:\n",
    "            datemax = max(x)\n",
    "\n",
    "        ax[index].set_title(key)\n",
    "        ax[index].set(ylabel='sequence matcher ration of the documents', xlabel='date of the document')\n",
    "        ax[index].xaxis.set_major_locator(years)\n",
    "        ax[index].xaxis.set_major_formatter(years_fmt)\n",
    "        ax[index].xaxis.set_minor_locator(year)\n",
    "        ax[index].set_xlim(datemin, datemax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_pairs = [('protocol_en_html','protocol_en_pdf'), ('protocol_de_html','protocol_de_pdf'), ('word_protocol_en_html','word_protocol_en_pdf'), ('word_protocol_de_html','word_protocol_de_pdf')]\n",
    "\n",
    "results = compute_similiarities(rule_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text similiarity between file formats\n",
    "\n",
    "Using ```SequenceMatcher.real_quick_ratio()``` as a heuristic for documents shows relatively good sequence matching between the extracted texts from the different file formats (mind the x-axis caused by still running .pdf-postprocessing). Outliers to 0 are caused by unreadable .pdf-files. Further investigation is needed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_text_similiarity(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping design options\n",
    "\n",
    "### Choosing the best format\n",
    "\n",
    "We have to assume that .pdf and .html-files contain the same information, therefore we can assume that the extracted text from .html-files is more reliable as the text extracted from .pdf files.\n",
    "\n",
    "We therefore index only one variant, preferably .html but falling back to .pdf using only one language\n",
    "```\n",
    "{\n",
    "    content: \"Alle meine Entchen...\",\n",
    "    language: \"DE\",\n",
    "    filetype: \".html\",\n",
    "    session_date: \"01.01.2022\",\n",
    "    document_type: \"protocol\"\n",
    "}\n",
    "```\n",
    "\n",
    "### Denormalization\n",
    "\n",
    "#### Choosing all formats\n",
    "\n",
    "We do not want to make assumptions about file content. We therefore index all available file variants in only one language.\n",
    "```\n",
    "{\n",
    "    content_html: \"Alle meine Entchen...\",\n",
    "    content_pdf: \"Heute back ich, morgen brau ich,...\",\n",
    "    language: \"DE\",\n",
    "    session_date: \"01.01.2022\",\n",
    "    document_type: \"protocol\"\n",
    "}\n",
    "```\n",
    "\n",
    "#### Choosing all formats and languages\n",
    "\n",
    "We do not want to make assumptions about file content and want to make queries in multiple languages \n",
    "```\n",
    "{\n",
    "    content_de_html: \"Alle meine Entchen...\",\n",
    "    content_en_html: \"All my little ducklings...\",\n",
    "    content_de_pdf: \"Heute back ich, morgen brau ich,...\",\n",
    "    content_en_pdf: \"Today I bake, tomorrow i brew, ...\",\n",
    "    session_date: \"01.01.2022\",\n",
    "    document_type: \"protocol\"\n",
    "}\n",
    "```\n",
    "\n",
    "### Nested\n",
    "\n",
    "Nesting documents into a parent documents. Comes with query limitations and every subdocument is stored in the same Lucene block (shouldn't be a problem because we aren't changing data after the initial indexing).\n",
    "\n",
    "```\n",
    "{\n",
    "    session_date: \"01.01.2022\",\n",
    "    document_type: \"protocol\",\n",
    "    docs:[\n",
    "        {\n",
    "            content: \"Alle meine Entchen...\",\n",
    "            language: \"DE\",\n",
    "            filetype: \".html\",\n",
    "        },\n",
    "        {\n",
    "            content: \"All my little ducklings...\",\n",
    "            language: \"EN\",\n",
    "            filetype: \".html\",\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "### Parent/Child\n",
    "\n",
    "Two different types of parent- and child-documents, letting Elasticsearch manage the relationship between them. Lower query performance and higher memory usage \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pros and Cons \n",
    "\n",
    "List of pros and cons as provided from [Elasticsearch](https://www.elastic.co/de/blog/managing-relations-inside-elasticsearch):\n",
    "\n",
    "#### Nested\n",
    "- Nested docs are stored in the same Lucene block as each other, which helps read/query performance. Reading a nested doc is faster than the equivalent parent/child.\n",
    "- Updating a single field in a nested document (parent or nested children) forces ES to reindex the entire nested document. This can be very expensive for large nested docs\n",
    "- \"Cross referencing\" nested documents is impossible\n",
    "- Best suited for data that does not change frequently\n",
    "\n",
    "#### Parent/Child\n",
    "\n",
    "- Children are stored separately from the parent, but are routed to the same shard. So parent/children are slightly less performance on read/query than nested\n",
    "- Parent/child mappings have a bit extra memory overhead, since ES maintains a \"join\" list in memory\n",
    "- Updating a child doc does not affect the parent or any other children, which can potentially save a lot of indexing on large docs\n",
    "- Sorting/scoring can be difficult with Parent/Child since the Has Child/Has Parent operations can be opaque at times\n",
    "\n",
    "#### Denormalization\n",
    "\n",
    "- You get to manage all the relations yourself!\n",
    "- Most flexible, most administrative overhead\n",
    "- May be more or less performant depending on your setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}